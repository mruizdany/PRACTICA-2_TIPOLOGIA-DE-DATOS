---
title: 'PRACTICA2: Analisis de accidentes de transito en Bogotá'
author: "Autor: Daniel Leonardo Martinez Ruiz"
date: "Enero 2025"
output:
  pdf_document:
    highlight: zenburn
    toc: yes
  word_document: default
  html_document:
    highlight: default
    number_sections: yes
    theme: cosmo
    toc: yes
    toc_depth: 2
editor_options: 
  markdown: 
    wrap: 72
  chunk_output_type: console
---

# 'PRACTICA 2: Análisis de accidentes de transito en Bogotá'

## Introducción

Los accidentes de tránsito constituyen un grave problema de salud
pública a nivel mundial, causando un número significativo de muertes y
lesiones cada año. En ciudades como Bogotá, la creciente motorización y
la urbanización han exacerbado esta problemática, generando una
necesidad urgente de implementar medidas para mejorar la seguridad vial.

## Objetivos Especifícos

El presente estudio tiene como objetivo principal identificar los
factores de riesgo asociados a los accidentes de tránsito en Bogotá y
localizar las zonas de mayor vulnerabilidad en la ciudad.

A través del análisis de un conjunto de datos detallados sobre
accidentes de tránsito, se busca contribuir al conocimiento sobre las
causas de los siniestros viales y proporcionar información valiosa para
la toma de decisiones en materia de seguridad vial.

Objetivos Específicos:

-   Identificar las características de los vehículos, conductores y vías
    involucradas en los accidentes con mayor frecuencia.

-   Evaluar la relación entre la gravedad de los accidentes y factores
    como la hora del día, el día de la semana y las condiciones
    climáticas.

-   Localizar los puntos negros de accidentalidad en la ciudad.

-   Proponer recomendaciones para mejorar la seguridad vial en Bogotá.

Este estudio se centrará en el análisis de datos de accidentes de
tránsito ocurridos en Bogotá durante el período comprendido en el
año2023. El área de estudio se limitará al territorio urbano de la
ciudad.

## Justificación

Comprender las causas de los accidentes de tránsito es fundamental para
diseñar e implementar estrategias efectivas de prevención. Los
resultados de este estudio permitirán identificar las áreas prioritarias
para la intervención, optimizar el uso de los recursos y reducir el
número de víctimas de accidentes de tránsito en Bogotá.

#### Estructura del Documento

En las siguientes secciones se presentará el marco teórico, los
materiales y métodos utilizados, los resultados obtenidos y una
discusión de los hallazgos. Finalmente, se presentarán las conclusiones
y recomendaciones para futuras investigaciones.

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r setup, include=FALSE}
# Instalar y cargar los paquetes necesarios
install.packages("dbscan")
install.packages(c("rpart", "rpart.plot", "caret"))

# Cargar librerías necesarias
library(readxl)
library(tidyverse)
library(cluster)
library(factoextra)
library(scales)
library(readxl)
library(caret)
library(dplyr)
library(dbscan)

library(dplyr)
library(tidyr)
library(janitor)
library(ggplot2)
library(gt)
library(lubridate)

# Leer datos
# Carga de datos
siniestros <- read_excel(ruta_archivo, sheet = "Siniestros")
vehiculos <- read_excel(ruta_archivo, sheet = "Vehiculos")
actor_vial <- read_excel(ruta_archivo, sheet = "Actor_vial")

# Exploración inicial
skim(siniestros)
skim(vehiculos)
skim(actor_vial)
```

## Materiales y Métodos

La siguiente tabla presenta una descripción detallada de lalguna de
variables utilizadas en este estudio. Cada variable se clasifica según
su tipo (numérica, categórica, ordinal) y se describe su significado e
importancia para el análisis.

Las variables espaciales (longitud y latitud) permiten realizar análisis
geográficos para identificar patrones espaciales en la ocurrencia de
accidentes.

Las variables relacionadas con los vehículos y los actores viales
proporcionan información sobre las características de los vehículos
involucrados y las personas afectadas.

Finalmente, las variables de identificación permiten relacionar los
diferentes registros y realizar un análisis más completo.

```{r setup, include=FALSE}

library(gtable)
library(dplyr)

# Crear un data frame con la información de las variables
variables <- data.frame(
  Variable = c("Fecha_Acc", "Hora_Accidente", "Dia_Semana_Acc", "Longitud", "Latitud", "Direccion", "Zona", "Clase", "Servicio", "Sistema_Transporte", "Rol_Actor", "Condicion", "Edad", "Genero", "Codigo_Accidente", "Codigo_Vehiculo", "Codigo_Accidentado", "Condicion_Climatica", "Estado_Via", "Tipo_Siniestro"),
  Tipo = c("Categórica (Fecha)", "Continua", "Categórica Ordinal", "Continua", "Continua", "Descriptiva", "Categórica", "Categórica", "Categórica", "Binaria", "Categórica", "Categórica Ordinal", "Continua", "Categórica", "Nominal", "Nominal", "Nominal", "Categorical", "Categorical", "Categorical"),
  Descripción = c(
    "Fecha exacta del accidente",
    "Hora del día en que ocurrió el siniestro",
    "Día de la semana en que ocurrió el accidente",
    "Coordenada geográfica que indica la distancia en grados decimales con respecto al meridiano principal",
    "Coordenada geográfica que indica la distancia en grados decimales con respecto a la línea del Ecuador",
    "Dirección completa donde ocurrió el siniestro vial",
    "Barrio o zona de siniestro",
    "Tipo de vehículo involucrado",
    "Clase de servicio proporcionado por el vehículo",
    "Indica si el vehículo pertenece a un sistema de transporte integrado como SITP",
    "Rol del actor vial en el accidente",
    "Resultado del accidente para el actor vial",
    "Edad del actor vial en años",
    "Género del actor vial",
    "Identificador único del accidente",
    "Identificador único del vehículo",
    "Identificador único del actor vial",
    "Condiciones climáticas al momento del accidente",
    "Estado de la vía al momento del accidente",
    "Tipo de siniestro"
  ),
  Importancia = c(
    "Permite analizar patrones temporales",
    "Identificar alta de accidentabilidad durante el día",
    "Ayuda a identificar días con mayor incidencia",
    "Análisis geoespacial",
    "Análisis geoespacial",
    "Ubicación específica",
    "Análisis zonal",
    "Características del vehículo",
    "Tipo de servicio",
    "Identificación de transporte público",
    "Rol en el accidente",
    "Gravedad del accidente",
    "Grupos vulnerables",
    "Diferencias de género",
    "Unir tablas",
    "Unir tablas",
    "Unir tablas",
    "Condiciones ambientales",
    "Estado de la vía",
    "Tipo de siniestro"
  )
)

# Crear la tabla con gt
variables %>%
  gt() %>%
  cols_label(
    Variable = "Variable",
    Tipo = "Tipo de Variable",
    Descripción = "Descripción",
    Importancia = "Importancia"
  ) %>%
  tab_header(
    title = "Descripción de las Variables"
  ) %>%
  tab_footnote(
    label = "Fuente:",
    content = "Base de datos de accidentes de tránsito de Bogotá"
  ) %>%
  gtsave("tabla_variables.html")
```

-   Variables espaciales: Estas variables son fundamentales para
    identificar zonas de alto riesgo y diseñar intervenciones
    específicas para cada área.
-   Variables de vehículos: Al analizar las características de los
    vehículos involucrados, podemos identificar los tipos de vehículos
    más propensos a estar involucrados en accidentes y diseñar políticas
    de seguridad vial específicas para cada tipo de vehículo.
-   Variables de actores viales: Al analizar las características de los
    conductores, pasajeros y peatones, podemos identificar los grupos
    más vulnerables y diseñar programas de educación vial y prevención
    de accidentes dirigidos a estos grupos.
-   Variables de contexto: Las variables relacionadas con las
    condiciones climáticas y el estado de la vía permiten evaluar el
    impacto de estos factores en la ocurrencia de accidentes.

#### Limpieza de Datos

-   **Valores faltantes:**

    -   Identificar las variables con valores faltantes.
    -   Decidir cómo tratarlos (eliminar registros, imputación por
        media, mediana o moda, o utilizar técnicas de imputación más
        sofisticadas).

-   **Valores atípicos:**

    -   Identificar y tratar los valores atípicos (outliers) que puedan
        distorsionar los resultados del análisis.
    -   Utilizar diagramas de caja (boxplots) o técnicas de detección de
        outliers para identificarlos.

-   **Consistencia de los datos:**

    -   Verificar la consistencia de los datos, por ejemplo, asegurando
        que las fechas estén en el formato correcto y que no haya
        valores duplicados.

-   **Categorización de variables:**

    -   Agrupar categorías poco frecuentes o crear nuevas categorías
        para simplificar el análisis.

```{r setup, include=FALSE}

# Función general de limpieza de datos
limpiar_datos <- function(dataset) {
  dataset %>%
    # Eliminar columnas completamente vacías
    select(where(~ !all(is.na(.)))) %>%
    # Renombrar columnas para evitar espacios y hacerlas amigables
    clean_names() %>%
    # Convertir tipos de datos específicos
    mutate(across(where(is.character), as.factor))
}

# Función para detectar y tratar outliers antes de eliminar filas con NA
detectar_outliers <- function(x) {
  z_scores <- scale(x)
  outliers <- which(abs(z_scores) > 3)
  x[outliers] <- max(x[-outliers], na.rm = TRUE)
  return(x)
}

# Función para imputar valores faltantes en columnas numéricas
gestionar_na <- function(dataset) {
  dataset %>%
    mutate(across(where(is.numeric), ~ ifelse(is.na(.), median(., na.rm = TRUE), .)))
}

# Función para crear nuevas variables
crear_variables <- function(dataset) {
  dataset %>%
    mutate(
      fecha_accidente = ymd(fecha_accidente),
      hora_accidente = hour(fecha_accidente),
      dia_semana_acc = wday(fecha_accidente, label = TRUE),
      fin_de_semana = ifelse(dia_semana_acc %in% c("Sábado", "Domingo"), 1, 0)
    )
}

# Limpieza inicial de los datasets
siniestros <- limpiar_datos(siniestros)
vehiculos <- limpiar_datos(vehiculos)
actor_vial <- limpiar_datos(actor_vial)

# Consolidación de los datos usando inner join para mayor limpieza
consolidado <- siniestros %>%
  inner_join(vehiculos, by = "codigo_accidente") %>%
  inner_join(actor_vial, by = c("codigo_accidente", "codigo_vehiculo"))

# Detectar y tratar outliers antes de imputar valores faltantes
consolidado <- consolidado %>%
  mutate(across(where(is.numeric), detectar_outliers))

# Imputar valores faltantes
consolidado <- gestionar_na(consolidado)

# Crear variables adicionales
consolidado <- crear_variables(consolidado)

# Normalizar datos numéricos
numericas_normalizadas <- consolidado %>%
  select(where(is.numeric)) %>%
  scale()

# Reconstruir el dataset final con datos categóricos y normalizados
final <- cbind(consolidado %>% select(-where(is.numeric)), numericas_normalizadas)

# Visualización de outliers y z-scores
# Ejemplo: Edad
if("edad" %in% names(final)) {
  ggplot(final, aes(x = edad)) +
    geom_boxplot() +
    labs(title = "Distribución de la edad después de tratar outliers")
}

# Resumen de las variables
variables <- data.frame(
  Variable = names(final),
  Tipo = sapply(final, class),
  Descripción = "Descripción por definir"  # Agregar descripciones específicas según sea necesario
)

# Mostrar tabla resumen con gt
variables %>%
  gt() %>%
  tab_header(
    title = "Descripción de las variables"
  ) %>%
  fmt_number(
    columns = where(is.numeric),
    decimals = 2
  ) %>%
  tab_style(
    style = cell_text(weight = "bold"),
    locations = cells_column_labels(everything())
  )

```

## Analisis de los datos detallada 

### Análisis de Kvecinos y aArboles de desición   

```{r setup, include=FALSE}
# Función para calcular WSS (Within Sum of Squares)
wss <- function(k) {
  kmeans(datos_norm, k, nstart = 25)$tot.withinss
}

# Calcular WSS para k = 1-10
k.values <- 1:10
wss_values <- map_dbl(k.values, wss)

# Visualizar el método del codo
pdf("elbow_plot.pdf")
plot(k.values, wss_values,
     type = "b", pch = 19, frame = FALSE,
     xlab = "Número de clusters (k)",
     ylab = "Suma total de cuadrados dentro del cluster",
     main = "Método del Codo para K óptimo")
dev.off()

# Aplicar k-means con datos normalizados (asumiendo k=4 basado en el método del codo)
set.seed(123)
km_norm <- kmeans(datos_norm, centers = 4, nstart = 25)

# Aplicar k-means con datos originales
km_orig <- kmeans(datos_original, centers = 4, nstart = 25)

# Añadir clusters a los datos originales
siniestros$cluster_norm <- km_norm$cluster
siniestros$cluster_orig <- km_orig$cluster

# Visualización de clusters (usando longitud y latitud)
ggplot(siniestros, aes(x = Longitud, y = Latitud, color = factor(cluster_norm))) +
  geom_point(alpha = 0.5) +
  theme_minimal() +
  labs(title = "Clusters de Siniestros (Datos Normalizados)",
       color = "Cluster") +
  scale_color_brewer(palette = "Set1")
ggsave("clusters_map.pdf")

# Análisis de características de clusters
cluster_summary <- siniestros %>%
  group_by(cluster_norm) %>%
  summarise(
    n = n(),
    avg_hora = mean(Hora_Acc),
    avg_minuto = mean(Min_Acc),
    avg_longitud = mean(Longitud),
    avg_latitud = mean(Latitud)
  )

# Guardar resumen en archivo
write.csv(cluster_summary, "cluster_summary.csv")

# Calidad del clustering
# Silhouette score para datos normalizados
sil <- silhouette(km_norm$cluster, dist(datos_norm))
pdf("silhouette_plot.pdf")
plot(sil, main = "Silhouette Plot")
dev.off()

# Prints para análisis
print("Resumen de clusters:")
print(cluster_summary)
print("\nInformación de k-means normalizado:")
print(km_norm$centers)
print("\nSilhouette score promedio:")
print(mean(sil[,3]))

# Aplicar PAM (Partitioning Around Medoids - equivalente a k-medians)
set.seed(123)  # Para reproducibilidad
km_medoids <- pam(datos_norm, k = 4)

# Crear resumen de clusters para k-medoids
cluster_summary_medoids <- datos_cluster %>%
  mutate(Cluster = km_medoids$clustering) %>%
  group_by(Cluster) %>%
  summarise(
    n = n(),
    avg_hora = median(Hora_Acc),
    avg_minuto = median(Min_Acc),
    avg_longitud = median(Longitud),
    avg_latitud = median(Latitud)
  )

# Visualización de clusters k-medoids
ggplot(data.frame(
  Longitud = datos_cluster$Longitud,
  Latitud = datos_cluster$Latitud,
  Cluster = as.factor(km_medoids$clustering)
)) +
  geom_point(aes(x = Longitud, y = Latitud, color = Cluster), alpha = 0.5) +
  theme_minimal() +
  labs(title = "Clusters de Siniestros (K-medoids)",
       x = "Longitud",
       y = "Latitud") +
  scale_color_brewer(palette = "Set1")

# Imprimir resúmenes y comparaciones
print("Resumen de clusters k-medoids:")
print(cluster_summary_medoids)

# Calcular el índice de Silhouette para k-medoids
sil_medoids <- silhouette(km_medoids)
print("\nSilhouette score promedio para k-medoids:")
print(mean(sil_medoids[,3]))

# Comparar las asignaciones de clusters entre k-means y k-medoids
comparison_table <- table(KMeans = km_norm$cluster, KMedoids = km_medoids$clustering)
print("\nTabla de comparación entre k-means y k-medoids:")
print(comparison_table)

# Alternativa más simple: Calcular matriz de distancias Manhattan
dist_manhattan <- dist(datos_norm, method = "manhattan")

# Aplicar k-means con esta matriz
km_manhattan <- kmeans(datos_norm, centers = 4)

# Ver resultados básicos
print("Tamaño de los clusters:")
print(km_manhattan$size)

print("\nCentros de los clusters:")
print(km_manhattan$centers)

# Comparar con k-means original
comparison <- table(
  Euclidean = km_norm$cluster, 
  Manhattan = km_manhattan$cluster
)
print("\nTabla de comparación:")
print(comparison)

# Crear visualizaciones para DBSCAN
plot_dbscan_1 <- ggplot(data.frame(
  Longitud = datos_cluster$Longitud,
  Latitud = datos_cluster$Latitud,
  Cluster = as.factor(dbscan_1$cluster)
)) +
  geom_point(aes(x = Longitud, y = Latitud, color = Cluster), alpha = 0.5) +
  theme_minimal() +
  labs(title = "Clusters DBSCAN (eps=0.5, minPts=5)")

print(plot_dbscan_1)

# Resumen DBSCAN
dbscan_summary_1 <- data.frame(
  datos_cluster,
  Cluster = dbscan_1$cluster
) %>%
  group_by(Cluster) %>%
  summarise(
    n = n(),
    avg_hora = mean(Hora_Acc),
    avg_minuto = mean(Min_Acc),
    avg_longitud = mean(Longitud),
    avg_latitud = mean(Latitud)
  )

# Imprimir resultados
print("Resumen DBSCAN (eps=0.5, minPts=5):")
print(dbscan_summary_1)

print("\nResumen DBSCAN (eps=0.3, minPts=10):")
print(table(dbscan_2$cluster))

# Comparación con k-means
print("\nComparación de número de clusters:")
print(paste("K-means clusters:", length(unique(km_norm$cluster))))
print(paste("DBSCAN_1 clusters:", length(unique(dbscan_1$cluster[dbscan_1$cluster != 0]))))
print(paste("DBSCAN_2 clusters:", length(unique(dbscan_2$cluster[dbscan_2$cluster != 0]))))

# Definir la semilla para reproducibilidad
set.seed(123)

# Calcular el número de observaciones para cada conjunto
n_total <- nrow(datos_cluster)
proporcion_train <- 0.8  # 80% para entrenamiento
n_train <- floor(n_total * proporcion_train)
n_test <- n_total - n


# Crear índices aleatorios para la selección
indices <- sample(1:n_total, n_total, replace = FALSE)
indices_train <- indices[1:n_train]
indices_test <- indices[(n_train + 1):n_total]

# Crear los conjuntos de entrenamiento y test
train_set <- datos_cluster[indices_train, ]
test_set <- datos_cluster[indices_test, ]

# Verificar las dimensiones
print("Dimensiones del conjunto de entrenamiento:")
print(dim(train_set))
print("Dimensiones del conjunto de test:")
print(dim(test_set))

# Verificar la distribución de los datos
summary_train <- summary(train_set)
summary_test <- summary(test_set)

print("Resumen del conjunto de entrenamiento:")
print(summary_train)
print("Resumen del conjunto de test:")
print(summary_test)

# Cargar librerías
library(rpart)
library(rpart.plot)
library(caret)

#Que variables tenemos disponibles
print("Variables disponibles en el conjunto de datos:")
names(train_set)

# Predicciones en conjunto de prueba
pred_sin_poda <- predict(arbol_sin_poda, test_set, type = "class")
pred_con_poda <- predict(arbol_con_poda, test_set, type = "class")

# Matrices de confusión
conf_matrix_sin_poda <- confusionMatrix(pred_sin_poda, test_set$Hora_Categoria)
conf_matrix_con_poda <- confusionMatrix(pred_con_poda, test_set$Hora_Categoria)

# Imprimir resultados
print("Matriz de confusión - Árbol sin poda:")
print(conf_matrix_sin_poda)

print("\nMatriz de confusión - Árbol con poda:")
print(conf_matrix_con_poda)

# Ver la importancia de las variables
print("\nImportancia de las variables - Árbol sin poda:")
print(arbol_sin_poda$variable.importance)

print("\nImportancia de las variables - Árbol con poda:")
print(arbol_con_poda$variable.importance)

# Obtener las reglas del árbol
print("\nReglas del árbol sin poda:")
printcp(arbol_sin_poda)

print("\nReglas del árbol con poda:")
printcp(arbol_con_poda)

# Visualización mejorada de los árboles
pdf("arbol_sin_poda.pdf")
rpart.plot(arbol_sin_poda, main="Árbol sin poda", 
           extra=104, # mostrar porcentajes
           box.palette="Blues", # esquema de colores azules
           shadow.col="gray") 
dev.off()

pdf("arbol_con_poda.pdf")
rpart.plot(arbol_con_poda, main="Árbol con poda",
           extra=104,
           box.palette="Blues",
           shadow.col="gray")
dev.off()

# Crear variable de franja horaria
datos <- datos %>%
  mutate(franja_horaria = case_when(
    Hora_Acc >= 0 & Hora_Acc < 6 ~ "Madrugada",
    Hora_Acc >= 6 & Hora_Acc < 12 ~ "Mañana",
    Hora_Acc >= 12 & Hora_Acc < 18 ~ "Tarde",
    Hora_Acc >= 18 & Hora_Acc <= 23 ~ "Noche",
    TRUE ~ "Otro"
  ))

# Verificar distribución de franjas horarias
print("Distribución de franjas horarias:")
print(table(datos$franja_horaria))

# Preparación de datos para modelado
# Seleccionar variables predictoras
datos_modelo <- datos %>%
  select(Longitud, Latitud, franja_horaria)

# Conversión de franja_horaria a factor
datos_modelo$franja_horaria <- as.factor(datos_modelo$franja_horaria)

# División de datos
set.seed(123)
indices <- createDataPartition(datos_modelo$franja_horaria, p = 0.8, list = FALSE)
datos_entrenamiento <- datos_modelo[indices, ]
datos_test <- datos_modelo[-indices, ]

# Verificar distribución en entrenamiento y test
print("Distribución en entrenamiento:")
print(table(datos_entrenamiento$franja_horaria))
print("Distribución en test:")
print(table(datos_test$franja_horaria))

# Modelo Random Forest
library(randomForest)

modelo_rf <- randomForest(franja_horaria ~ Longitud + Latitud, 
                          data = datos_entrenamiento, 
                          ntree = 500, 
                          importance = TRUE)

# Predicciones
predicciones_rf <- predict(modelo_rf, datos_test)

# Matriz de Confusión
conf_matrix_rf <- confusionMatrix(predicciones_rf, datos_test$franja_horaria)
print("Matriz de Confusión Random Forest:")
print(conf_matrix_rf)

# Gráfico de Importancia de Variables
pdf("importancia_variables_rf.pdf")
varImpPlot(modelo_rf, main = "Importancia de Variables - Random Forest")
dev.off()

# Exportar resultados
write.csv(conf_matrix_rf$table, "matriz_confusion_rf.csv")
```

## Conculsiones

-   **Identificación de factores de riesgo:** Se podrían identificar los
    principales factores que contribuyen a la ocurrencia de accidentes,
    como las condiciones climáticas, el estado de las vías, la hora del
    día, el tipo de vehículo y el comportamiento del conductor.

-   **Localización de zonas de alto riesgo:** Se podrían identificar las
    zonas de la ciudad con mayor concentración de accidentes, lo que
    permitiría enfocar las medidas de seguridad vial en áreas
    específicas.

-   **Análisis de patrones temporales:** Se podrían identificar patrones
    temporales en la ocurrencia de accidentes, como por ejemplo, si hay
    más accidentes durante ciertos días de la semana o en determinadas
    horas del día.

-   **Evaluación de políticas de seguridad vial:** Se podría evaluar el
    impacto de políticas de seguridad vial implementadas en el pasado,
    como la instalación de semáforos o la construcción de ciclovías.

-   **Identificación de grupos vulnerables:** Se podrían identificar los
    grupos de población más vulnerables a sufrir accidentes, como los
    peatones, los ciclistas o los conductores de motocicletas.
